# get an equal number of samples between the different
# levels of the type variable, we group_by that factor    first
# https://stackoverflow.com/a/23832910/5420796
group_by(tipo) %>%
sample_n(30) %>%
# Add new rows to data set for analysis
mutate(tipoVerbo = "", tipoParOne = "", tipoParTwo = "", parOne = "", parTwo = "") %>%
# Use select() to rearrange the rows
select(izquierda, tipoParOne, parOne, kwic, tipoVerbo, tipoParTwo, parTwo, derecha, tipo, area, everything())
datosCount <- datos %>%
count(tipo)
datosForo <- datos %>%
#bind_rows(datos, datosPiloto) %>%
filter(tipo %in% c("Foro", "Científico")) %>%
# Before taking a sample of the data frame if we want to
# get an equal number of samples between the different
# levels of the type variable, we group_by that factor    first
# https://stackoverflow.com/a/23832910/5420796
group_by(tipo) %>%
sample_n(30) %>%
# Add new rows to data set for analysis
mutate(tipoVerbo = "", tipoParOne = "", tipoParTwo = "", parOne = "", parTwo = "") %>%
# Use select() to rearrange the rows
select(izquierda, tipoParOne, parOne, kwic, tipoVerbo, tipoParTwo, parTwo, derecha, tipo, area, everything())
# Leer los datos del
datos <- read_csv("20181101_130322.csv")
# First did CQL for main verbs in Sketch Engine's concordancer, selected a ramdom sample, and manually made tidy
# Read in Pilot Data
datosPiloto <- read_csv("concordance_user_maxman100_piloto_20181106043739.csv")
datosPilotoTest <- datosPiloto %>%
rename(url = colTwo, izquierda = colThree, kwic = colFour, derecha = colFive) %>%
# Select all pertinent rows
select(url, izquierda, kwic, derecha) %>%
mutate(tipoVerbo = "", tipoParOne = "", tipoParTwo = "", parOne = "", parTwo = "") %>%
select(izquierda, tipoParOne, parOne, kwic, tipoVerbo, tipoParTwo, parTwo, derecha, url, everything())
datosCount <- datos %>%
count(tipo)
datosForo <- datos %>%
#bind_rows(datos, datosPiloto) %>%
filter(tipo %in% c("Foro", "Científico")) %>%
# Before taking a sample of the data frame if we want to
# get an equal number of samples between the different
# levels of the type variable, we group_by that factor    first
# https://stackoverflow.com/a/23832910/5420796
group_by(tipo) %>%
sample_n(30) %>%
# Add new rows to data set for analysis
mutate(tipoVerbo = "", tipoParOne = "", tipoParTwo = "", parOne = "", parTwo = "") %>%
# Use select() to rearrange the rows
select(izquierda, tipoParOne, parOne, kwic, tipoVerbo, tipoParTwo, parTwo, derecha, tipo, area, everything())
View(datosForo)
bind_rows(datosForo, datosPilotoTest)
new <- bind_rows(datosForo, datosPilotoTest)
View(new)
# Leer los datos del
datos <- read_csv("20181101_130322.csv")
# First did CQL for main verbs in Sketch Engine's concordancer, selected a ramdom sample, and manually made tidy
# Read in Pilot Data
datosPiloto <- read_csv("concordance_user_maxman100_piloto_20181106043739.csv")
# Leer los datos del
datos <- read_csv("20181101_130322.csv")
# First did CQL for main verbs in Sketch Engine's concordancer, selected a ramdom sample, and manually made tidy
# Read in Pilot Data
datosPiloto <- read_csv("concordance_user_maxman100_piloto_20181106043739.csv")
datosPilotoTest <- datosPiloto %>%
rename(url = colTwo, izquierda = colThree, kwic = colFour, derecha = colFive) %>%
# Select all pertinent rows
select(url, izquierda, kwic, derecha) %>%
mutate(tipoVerbo = "", tipoParOne = "", tipoParTwo = "", parOne = "", parTwo = "") %>%
select(izquierda, tipoParOne, parOne, kwic, tipoVerbo, tipoParTwo, parTwo, derecha, url, everything())
datosCount <- datos %>%
count(tipo)
datosForo <- datos %>%
#bind_rows(datos, datosPiloto) %>%
filter(tipo %in% c("Foro", "Científico")) %>%
# Before taking a sample of the data frame if we want to
# get an equal number of samples between the different
# levels of the type variable, we group_by that factor    first
# https://stackoverflow.com/a/23832910/5420796
group_by(tipo) %>%
sample_n(30) %>%
# Add new rows to data set for analysis
mutate(tipoVerbo = "", tipoParOne = "", tipoParTwo = "", parOne = "", parTwo = "") %>%
# Use select() to rearrange the rows
select(izquierda, tipoParOne, parOne, kwic, tipoVerbo, tipoParTwo, parTwo, derecha, tipo, area, everything())
# Combine Rows
datosCombinados <- bind_rows(datosForo, datosPilotoTest)
# Write the data as an xlsx file to the working directory
# The Tidyverse doesn't appear to have a writexl equivelant
openxlsx::write.xlsx(datosCombinados, file = "datos.xlsx")
# Leer los datos del
datos <- read_csv("20181101_130322.csv")
# First did CQL for main verbs in Sketch Engine's concordancer, selected a ramdom sample, and manually made tidy
# Read in Pilot Data
datosPiloto <- read_csv("concordance_user_maxman100_piloto_20181106043739.csv")
datosPiloto <- datosPiloto %>%
rename(url = colTwo, izquierda = colThree, kwic = colFour, derecha = colFive) %>%
# Select all pertinent rows
select(url, izquierda, kwic, derecha) %>%
mutate(tipoVerbo = "", tipoParOne = "", tipoParTwo = "", parOne = "", parTwo = "", tipo = "oral") %>%
select(izquierda, tipoParOne, parOne, kwic, tipoVerbo, tipoParTwo, parTwo, derecha, url, everything())
datosCount <- datos %>%
count(tipo)
datosForo <- datos %>%
#bind_rows(datos, datosPiloto) %>%
filter(tipo %in% c("Foro", "Científico")) %>%
# Before taking a sample of the data frame if we want to
# get an equal number of samples between the different
# levels of the type variable, we group_by that factor    first
# https://stackoverflow.com/a/23832910/5420796
group_by(tipo) %>%
sample_n(30) %>%
# Add new rows to data set for analysis
mutate(tipoVerbo = "", tipoParOne = "", tipoParTwo = "", parOne = "", parTwo = "") %>%
# Use select() to rearrange the rows
select(izquierda, tipoParOne, parOne, kwic, tipoVerbo, tipoParTwo, parTwo, derecha, tipo, area, everything())
# Combine Rows
datosCombinados <- bind_rows(datosForo, datosPiloto)
# Write the data as an xlsx file to the working directory
# The Tidyverse doesn't appear to have a writexl equivelant
openxlsx::write.xlsx(datosCombinados, file = "datos.xlsx")
```{r}
# Leer los datos del
datos <- read_csv("20181101_130322.csv")
# First did CQL for main verbs in Sketch Engine's concordancer, selected a ramdom sample, and manually made tidy
# Read in Pilot Data
datosPiloto <- read_csv("concordance_user_maxman100_piloto_20181106043739.csv")
datosPiloto <- datosPiloto %>%
rename(url = colTwo, izquierda = colThree, kwic = colFour, derecha = colFive) %>%
# Select all pertinent rows
select(url, izquierda, kwic, derecha) %>%
mutate(tipoVerbo = "", tipoParOne = "", tipoParTwo = "", parOne = "", parTwo = "", tipo = "oral") %>%
select(izquierda, tipoParOne, parOne, kwic, tipoVerbo, tipoParTwo, parTwo, derecha, url, everything())
datosCount <- datos %>%
count(tipo)
datosForo <- datos %>%
#bind_rows(datos, datosPiloto) %>%
filter(tipo %in% c("Foro", "Científico")) %>%
# Before taking a sample of the data frame if we want to
# get an equal number of samples between the different
# levels of the type variable, we group_by that factor    first
# https://stackoverflow.com/a/23832910/5420796
group_by(tipo) %>%
sample_n(30) %>%
# Add new rows to data set for analysis
mutate(tipoVerbo = "", tipoParOne = "", tipoParTwo = "", parOne = "", parTwo = "") %>%
# Use select() to rearrange the rows
select(izquierda, tipoParOne, parOne, kwic, tipoVerbo, tipoParTwo, parTwo, derecha, tipo, area, everything())
# Combine Rows
datosCombinados <- bind_rows(datosForo, datosPiloto)
# Write the data as an xlsx file to the working directory
# The Tidyverse doesn't appear to have a writexl equivelant
openxlsx::write.xlsx(datosCombinados, file = "datos.xlsx")
# https://www.r-bloggers.com/using-sqlite-in-r/
# https://github.com/r-dbi/RSQLite/issues/57#issuecomment-61533366
library("RSQLite")
library(DBI)
library(dplyr)
library(tm)
library(ggplot2)
# This method can be used to connect to another database online
# https://campus.datacamp.com/courses/importing-data-in-r-part-2/importing-data-from-databases-part-1?ex=1
con = dbConnect(RSQLite::SQLite(), dbname="/Users/maxcarey/Documents/proj/totago/totagoData.db")
# Read itenerary tabke if database into a dataframe
totagoData <- dbReadTable(con, "itenerary")
# Disconnect from database, close connection
dbDisconnect(con)
# See if there are any rows that are coded as not valid, but have both
totagoDataWrangled <- totagoData %>%
filter(valid == 1, postalCode == "none") %>%
group_by(startFromLocation) %>%
count() %>%
arrange(desc(n)) %>%
head(10) %>%
ggplot(aes(x = startFromLocation, y = n)) +
geom_bar(stat="identity")
%>%
ggplot(aes(x = startFromLocation, y = n)) +
geom_bar(stat="identity") +
theme(text = element_text(size=20), axis.text.x = element_text(angle = 90)) +
ggtitle("Top 10")
# See if there are any rows that are coded as not valid, but have both
totagoDataWrangled <- totagoData %>%
filter(valid == 1, postalCode == "none") %>%
group_by(startFromLocation) %>%
count() %>%
arrange(desc(n)) %>%
head(10) %>%
ggplot(.,aes(x = startFromLocation, y = n)) +
geom_bar(stat="identity")
ggplot(totagoDataWrangled,aes(x = startFromLocation, y = n)) +
geom_bar(stat="identity")
# See if there are any rows that are coded as not valid, but have both
totagoDataWrangled <- totagoData %>%
filter(valid == 1, postalCode == "none") %>%
group_by(startFromLocation) %>%
count() %>%
arrange(desc(n)) %>%
head(10)
ggplot(totagoDataWrangled,aes(x = startFromLocation, y = n)) +
geom_bar(stat="identity")
ggplot(totagoDataWrangled,aes(x = reorder(startFromLocation, -n), y = n)) +
geom_bar(stat="identity") +
theme(text = element_text(size=20), axis.text.x = element_text(angle = 90)) +
ggtitle("Top 10")
ggplot(totagoDataWrangled,aes(x = reorder(startFromLocation, -n), y = n)) +
geom_bar(stat="identity") +
theme(text = element_text(size=10), axis.text.x = element_text(angle = 90)) +
ggtitle("Top 10")
# See if there are any rows that are coded as not valid, but have both
totagoDataWrangled <- totagoData %>%
filter(valid == 1, postalCode == "none") %>%
group_by(startFromLocation) %>%
count() %>%
arrange(desc(n)) %>%
head(20)
ggplot(totagoDataWrangled,aes(x = reorder(startFromLocation, -n), y = n)) +
geom_bar(stat="identity") +
theme(text = element_text(size=10), axis.text.x = element_text(angle = 90)) +
ggtitle("Top 10")
# See if there are any rows that are coded as not valid, but have both
totagoDataWrangled <- totagoData %>%
filter(valid == 1, postalCode == "none") %>%
group_by(startFromLocation) %>%
count() %>%
arrange(desc(n)) %>%
head(20)
ggplot(totagoDataWrangled,aes(x = reorder(startFromLocation, -n), y = n)) +
geom_bar(stat="identity") +
theme(text = element_text(size=8), axis.text.x = element_text(angle = 90)) +
ggtitle("Top 10")
ggplot(totagoDataWrangled,aes(x = reorder(startFromLocation, -n), y = n)) +
geom_bar(stat="identity") +
theme(text = element_text(size=10), axis.text.x = element_text(angle = 90)) +
ggtitle("Top 20")
# https://www.r-bloggers.com/using-sqlite-in-r/
# https://github.com/r-dbi/RSQLite/issues/57#issuecomment-61533366
library("RSQLite")
library(DBI)
library(dplyr)
library(tm)
library(ggplot2)
# This method can be used to connect to another database online
# https://campus.datacamp.com/courses/importing-data-in-r-part-2/importing-data-from-databases-part-1?ex=1
con = dbConnect(RSQLite::SQLite(), dbname="/Users/maxcarey/Documents/proj/totago/totagoData.db")
# Read itenerary tabke if database into a dataframe
totagoData <- dbReadTable(con, "itenerary")
# Disconnect from database, close connection
dbDisconnect(con)
# See if there are any rows that are coded as not valid, but have both
totagoDataWrangled <- totagoData %>%
filter(valid == 1, postalCode == "none") %>%
group_by(startFromLocation) %>%
count() %>%
arrange(desc(n)) %>%
head(20)
ggplot(totagoDataWrangled,aes(x = reorder(startFromLocation, -n), y = n)) +
geom_bar(stat="identity") +
theme(text = element_text(size=10), axis.text.x = element_text(angle = 90)) +
ggtitle("Top 20")
View(totagoData)
# https://www.r-bloggers.com/using-sqlite-in-r/
# https://github.com/r-dbi/RSQLite/issues/57#issuecomment-61533366
library("RSQLite")
library(DBI)
library(dplyr)
library(tm)
library(ggplot2)
# This method can be used to connect to another database online
# https://campus.datacamp.com/courses/importing-data-in-r-part-2/importing-data-from-databases-part-1?ex=1
con = dbConnect(RSQLite::SQLite(), dbname="/Users/maxcarey/Documents/proj/totago/totagoData.db")
# Read itenerary tabke if database into a dataframe
totagoData <- dbReadTable(con, "itenerary")
# Disconnect from database, close connection
dbDisconnect(con)
# See if there are any rows that are coded as not valid, but have both
totagoDataWrangled <- totagoData %>%
filter(valid == 1, postalCode == "none") %>%
group_by(startFromLocation) %>%
count() %>%
arrange(desc(n)) %>%
head(20)
ggplot(totagoDataWrangled,aes(x = reorder(startFromLocation, -n), y = n)) +
geom_bar(stat="identity") +
theme(text = element_text(size=10), axis.text.x = element_text(angle = 90)) +
ggtitle("Top 20")
View(totagoData)
# https://www.r-bloggers.com/using-sqlite-in-r/
# https://github.com/r-dbi/RSQLite/issues/57#issuecomment-61533366
library("RSQLite")
library(DBI)
library(dplyr)
library(tm)
library(ggplot2)
# This method can be used to connect to another database online
# https://campus.datacamp.com/courses/importing-data-in-r-part-2/importing-data-from-databases-part-1?ex=1
con = dbConnect(RSQLite::SQLite(), dbname="/Users/maxcarey/Documents/proj/totago/totagoData.db")
# Read itenerary tabke if database into a dataframe
totagoData <- dbReadTable(con, "itenerary")
# Disconnect from database, close connection
dbDisconnect(con)
# See if there are any rows that are coded as not valid, but have both
totagoDataWrangled <- totagoData %>%
filter(valid == 1, postalCode == "none") %>%
group_by(startFromLocation) %>%
count() %>%
arrange(desc(n)) %>%
head(20)
ggplot(totagoDataWrangled,aes(x = reorder(startFromLocation, -n), y = n)) +
geom_bar(stat="identity") +
theme(text = element_text(size=10), axis.text.x = element_text(angle = 90)) +
ggtitle("Top 20")
View(totagoData)
setwd("~/Documents/tesis/datos/R_scripts/intermediate_scripts")
# Load packages and dependencies
library(tidyverse)
library(xtable)
library(knitr)
library(lme4)
library(party)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(texreg)
library(XML)
library(ggpmisc)
library(gganimate)
library(wordcloud2)
library(gridExtra)
library(readr)
library(kableExtra)
library(RefManageR)
source("../functions/getSex.R")
source("../functions/getGrupo.R")
source("../functions/getFrequency.R")
source("../functions/getFrequencyAbsolute.R")
source("../functions/createTable.R")
source("../functions/recodeVowel.R")
source("../functions/recodeStress.R")
source("../functions/removeAccents.R")
# Load the file via an HTML table
# This was a bit of a work around since I wanted to have a table on the table generator that can also
# be latex, for final analysis I should load this from survey respondant data
speakerSocialData <- data.frame(readHTMLTable("../../../misc/speakers.html", stringsAsFactors = FALSE, encoding = "UTF-8", header = TRUE))
colnames(speakerSocialData) <- c("grupo", "sexo", "edad", "educación", "tiempo en cdmx", "profesión")
# Read in data
dData <- read.delim("../../corpus/d/corrected/dData.txt")
# Create a new column uniqueID that consists of a concatenation between two fields
dData <- dData %>%
mutate(uniqueID = paste(Filename, midPoint, sep = '-'))
# This is where the joining of new data will begin to take place
#write.csv(dDataNoDip, file="../../datos_finales/dNoDip.csv", quote = FALSE, row.names = FALSE)
dDataFirstMerge <- read.delim("~/Documents/tesis/datos/corpus/d/corrected/secondpass/corrected/dData2.txt")
# Only select the columns that we care about
dDataFirstMerge <- dDataFirstMerge %>%
select(uniqueID,Status,stopBurst,formante,observableDip, intensityDif,stopMinInten,vowelMaxInten,startTime,endTime,midPoint,Notes,Date,Praat.Version,OS)
# Left join all these columsn
dData <- left_join(dData, dDataFirstMerge, by = "uniqueID")
# Use a bunch of mutate() and embedded ifelse() statements to update the data
# There is probably an easier way to do this
# See the blog below for examples, also these look like some
# really cool slides that I could use as well for my presentation in late Jan
# https://rstudio-pubs-static.s3.amazonaws.com/116317_e6922e81e72e4e3f83995485ce686c14.html#/
dData <- dData %>%
# String Vectors (that are actually factors)
mutate(Status.x = ifelse(!is.na(Status.y),as.character(Status.y),as.character(Status.x))) %>%
mutate(stopBurst.x = ifelse(!is.na(stopBurst.y),as.character(stopBurst.y),as.character(stopBurst.x))) %>%
mutate(formante.x = ifelse(!is.na(formante.y), as.character(formante.y), as.character(formante.x))) %>%
mutate(observableDip.x = ifelse(!is.na(observableDip.y), as.character(observableDip.y), as.character(observableDip.x))) %>%
mutate(formante.x = ifelse(!is.na(formante.y), as.character(formante.y), as.character(formante.x))) %>%
mutate(Notes.x = ifelse(!is.na(Notes.y), as.character(Notes.y), as.character(Notes.x))) %>%
mutate(Date.x = ifelse(!is.na(Date.y), as.character(Date.y), as.character(Date.x))) %>%
# Numerical vectors
mutate(intensityDif.x = ifelse(!is.na(intensityDif.y), intensityDif.y, intensityDif.x)) %>%
mutate(stopMinInten.x = ifelse(!is.na(stopMinInten.y), stopMinInten.y, stopMinInten.x)) %>%
mutate(vowelMaxInten.x = ifelse(!is.na(vowelMaxInten.y), vowelMaxInten.y, vowelMaxInten.x)) %>%
mutate(startTime.x = ifelse(!is.na(startTime.y), startTime.y, startTime.x)) %>%
mutate(endTime.x = ifelse(!is.na(endTime.y), endTime.y, endTime.x)) %>%
mutate(midPoint.x = ifelse(!is.na(midPoint.y), midPoint.y, midPoint.x)) %>%
# Drop all of the columns that end with ".y"
# See blog: https://blog.exploratory.io/selecting-columns-809bdd1ef615
select(-ends_with(".y")) %>%
# Drop the x suffix from all the the columns that were created with join
# https://stackoverflow.com/a/45960434/5420796
rename_at(.vars = vars(ends_with(".x")),
.funs = funs(sub("[.]x$", "", .))) %>%
# First add a value of one to the dependent variable since elided tokens will get a value of one
mutate(intensityDif = intensityDif + 1) %>%
# In the cases where observableDip is elidida make the intensity dip equal to 1
mutate(intensityDif = ifelse(observableDip == "elidida",1, intensityDif))
# Change back to factors
# https://stackoverflow.com/a/33180265/5420796
cols <- c("Status", "stopBurst", "formante", "observableDip")
dData[cols] <- lapply(dData[cols], factor)  ## as.factor() could also be used
# Recode previous vowel column using the recodeVowel function
dData$preceding_phone <- mapply(recodeVowel, dData$preceding_phone)
dData$preceding_phone <- as.factor(dData$preceding_phone)
# Recode following vowel column using the recodeVowel function
dData$following_phone <- mapply(recodeVowel, dData$following_phone)
dData$following_phone <- as.factor(dData$following_phone)
# Recode stress using the recode stress function
dData$stressStatus <- mapply(recodeStress, dData$stressStatus)
dData$stressStatus <- as.factor(dData$stressStatus)
# Filter out discounted tokens and rename some column names
dData <- dData %>%
# Rename columns
filter(Status != "discount") %>%
rename(speaker = Filename)
#Add sex variable
dData$sexo <- mapply(getSexo, dData$speaker)
dData$sexo <- as.factor(dData$sexo)
# Add group variable
dData$grupo <- mapply(getGrupo, dData$speaker)
dData$grupo <- as.factor(dData$grupo)
# Add frequency as a word frequency as a variable
# First read in csv file of all the words in the entire corpus, generated by Python
allWordList <- read.csv("~/Documents/tesis/datos/corpus/d/corrected/wordList.csv")
dData$wordFrequency <- mapply(getFrequency, dData$Word)
dData$wordFrequencyAbsolute <- mapply(getFrequencyAbsolute, dData$Word)
#dData$wordFrequency <- as.factor(dData$wordFrequency)
# This code below can be used to get the frequency of the words used in the subset of the database
#wordFrequency <- dData %>%
#  count(Word) %>%
#  arrange(desc(n))
# Rearrange columns
dData <- dData %>%
select(uniqueID, intensityDif, preceding_phone, Consonant, following_phone, grupo, sexo, Word, wordFrequency, stressStatus, wordPosition, pastParticiple, observableDip, stopBurst, formante, everything())
# This can be references from within the getFrequency() fucntion
wordFrequencyNoDip <- dData %>%
count(Word) %>%
arrange(desc(n))
# Get the number of tokens per speaker
tokensPerSpeaker <- dData %>%
filter(observableDip == "yes") %>%
count(speaker)
# Remove non observable dips
# But first create a different subset
dDataFULL <- dData
dData <- dData %>%
filter(observableDip != "no")
# Create subsets of each group for further analysis
dDataCDMXSITU <- dData %>%
filter(grupo == "CDMX-IN-SITU")
dDataCHICDMX <- dData %>%
filter(grupo == "CHI-EN-CDMX")
dDataCHISITU<- dData %>%
filter(grupo == "CHI-IN-SITU")
# Calculate Density peaks of the distributions
#http://ianmadd.github.io/pages/PeakDensityDistribution.html
peakYCDMXSITU <- which.max(density(dDataCDMXSITU$intensityDif)$y)
densityCDMXSITU <- density(dDataCDMXSITU$intensityDif)$x[peakYCDMXSITU]
peakYCHICDMX <- which.max(density(dDataCHICDMX$intensityDif)$y)
densityCHICDMX <- density(dDataCHICDMX$intensityDif)$x[peakYCHICDMX]
peakYCHISITU <- which.max(density(dDataCHISITU$intensityDif)$y)
densityCHISITU <- density(dDataCHISITU$intensityDif)$x[peakYCHISITU]
# Create tables to display in report
variableSocial <- c('Grupo', 'Sexo')
valores <- c('CDMX-IN-SITU, CHI-IN-SITU, CHI-EN-CDMX', 'masculino, femenino')
tipo <- c('categórico', 'categórico')
variablesSociales <- data.frame(variableSocial, valores, tipo)
variableLing <- c('Vocal previa', 'Vocal siguiente', 'tonicidad', 'frequencia de token')
valoresLing <- c('alto, medio, bajo', 'alto, medio, bajo', 'tónico, no tónico', 'log')
tipoLing <- c('categoórico', 'categórico', 'categórico', 'log (continuo)')
variableLings <- data.frame(variableLing, valoresLing, tipoLing)
nombres <- c("Intercept", "Vocal previa (baja)", "Vocal previa (media)", "Vocal siguiente (baja)", "Vocal siguiente (media)", "Tonicidad (tónica)", "Frecuencia de token", "CHI-EN-CDMX", "CHI-IN-SITU", "MASCULINO")
# Create the data used to generate the word clouds
# This will be done the good old fashion way
# Follow steps 2 and 4 of this tutorial: http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know
# Read in the vector dData$Word as a corpus object (uses the tm package)
myCorpus <- Corpus(VectorSource(dData$Word))
# Create a frequency column
# Looks this is done by gold old base R
dtm <- TermDocumentMatrix(myCorpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
# Remove row names from kabble column
# https://www.nesono.com/node/456
rownames(d) <- c()
# Relevel factors to be more intuitive in displays
dData$grupo <- factor(dData$grupo, levels = c("CHI-IN-SITU", "CHI-EN-CDMX", "CDMX-IN-SITU"))
dData$preceding_phone <- factor(dData$preceding_phone, levels = c("alta", "media", "baja"))
dData$following_phone <- factor(dData$following_phone, levels = c("alta", "media", "baja"))
# Calculate the median to display on teh ggplots
# https://stackoverflow.com/questions/19876505/boxplot-show-the-value-of-mean/19876655
medianPreceding_phone <- aggregate(intensityDif ~ preceding_phone + grupo, dData, median)
medianFollowing_phone <- aggregate(intensityDif ~  following_phone + grupo, dData, median)
medianStress <- aggregate(intensityDif ~  stressStatus + grupo, dData, median)
# Read in LVS tables
factores_aleatorios <- read_csv("~/Documents/tesis/datos/R_scripts/factores_aleatorios.csv")
factores_fijos <- read_csv("~/Documents/tesis/datos/R_scripts/factores_fijos.csv")
# Create a csv file to plug into language variation suite
#testME <- dData %>%
#  select(intensityDif, preceding_phone, following_phone, grupo, sexo, Word, wordFrequency, stressStatus, speaker) %>%
#  # Use mutate at to replace a new column
#  mutate_at(vars(Word), funs(removeAccents(Word))) %>%
#  write.csv(.,file = "~/Desktop/test.csv", row.names = FALSE)
#Render the RMarkDown Scriot
#rmarkdown::render("../descriptiveD.Rmd", output_dir = "../../../reportes/", output_file = "new.html")
#knitr::knit("../descriptiveD.Rmd", output = "new.html")
# Create a subset of the dataFrame to analyze
# The first thing we want to look at are the states
# dDataNoDip <- dDataFULL %>%
#  filter(observableDip %in% c("no", "maybe")) %>%
#  select(uniqueID,intensityDif,stressStatus,observableDip,stopBurst,formante, startTime, endTime, Status)
setwd("~/Documents/presentations")
xaringan::inf_mr()
options(htmltools.dir.version = FALSE)
knitr::include_graphics("../../tesis/datos/figures/ejemplos/frenadas.png")
setwd("~/Documents/presentations")
